{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3.2 Optimization II\n",
    "\n",
    "By the end of this notebook you should be able to:\n",
    "- Understand *Grid search* and *Random Search* methods of hyperparameter optimization\n",
    "  - and understand how these can be used in scikit-learn\n",
    "- Understand how *Genetic Algorithms* work, and how they may be used for feature selection\n",
    "- Other topics that may be briefly mentioned: \n",
    "  - Multi-objective optimization\n",
    "  - Bayesian optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Hyperparameter optimization\n",
    "\n",
    "We can now (hopefully) apply optimization algorithms to minimize a cost function in order to find the best parameters (weights) for our model\n",
    "\n",
    "But even if we minimize the cost function perfectly, there is still a chance these 'optimal' parameters could still lead to a poor model if the hyperparameters have not been adequately set\n",
    "- e.g. we could imagine setting the regularization parameter $\\lambda$ to an extremely high level, so that the loss function (e.g. L2 loss) is essentially ignored in the cost function, and instead we just find an extremely simple model that underfits\n",
    "\n",
    "To prevent this, we need to perform hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are three main methods usually considered for hyperparameter optimization:\n",
    "\n",
    "1. Grid Search\n",
    "2. Random Search\n",
    "3. Bayesian Optimization\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Grid Search\n",
    "\n",
    "Suppose we have two hyperparameters: a regularization parameter $\\lambda$ and a learning rate $\\alpha$\n",
    "\n",
    "Suppose we wish to limit each hyperparameter to just a small, finite set of values, e.g.:\n",
    "- $\\lambda \\in A = \\{0.001,0.01,0.1,0.5,1.0\\}$\n",
    "- $\\alpha \\in B = \\{0.01,0.1,0.5,1\\}$\n",
    "\n",
    "The total number of combinations of hyperparameter values that we could possibly consider then is $|A \\times B| = 20$.\n",
    " - As this is reasonably small, we could feasibly try out all of these combinations.\n",
    " - This approach is known as *grid search*.\n",
    " - The performance of each trained model would be measured using cross-validation (introduced in Week 2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Grid-search does not scale well if we have many hyperparameters with many values:\n",
    "- Suppose we had 6 hyperparameters, each with 10 values: that is one million different combinations of hyperparameter values\n",
    "\n",
    "Also, grid search is wasteful if certain hyperparameter values are not important to model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Random Search\n",
    "\n",
    "**Note: This is different from the random search used for optimizing model parameters**\n",
    "\n",
    "- Random search in the context of hyperparmeter optimization is even simpler: we just sample each hyperparameter randomly\n",
    "- Uniform and log-uniform distributions are often used to sample the hyperparameter values\n",
    "- Random search in this way is often found to be quite effective, and often better than grid seach, especially in higher dimensions\n",
    "\n",
    "![gridvrandom](Images/grid_vs_random_cite.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bayesian Optimization\n",
    "\n",
    "- Bayesian optimization is a powerful technique for efficient optimization\n",
    "- It has become a very popular technique for hyperparameter optimization in recent years\n",
    "- Now available in `scikit-optimize`: \n",
    "  - `from skopt import BayesSearchCV`\n",
    "- We will hopefully have some time to explore Bayesian Optimization in Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperparameter-tuning in scikit-learn \n",
    "\n",
    "Typical use in scikit-learn is shown in the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "param_grid = {'bootstrap': [True],\n",
    "     'max_depth': [6, 10],\n",
    "     'max_features': ['auto', 'sqrt'],\n",
    "     'min_samples_leaf': [3, 5],\n",
    "     'min_samples_split': [4, 6],\n",
    "     'n_estimators': [100, 350]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "forest_clf = RandomForestClassifier()\n",
    "\n",
    "forest_grid_search = GridSearchCV(forest_clf, param_grid, cv=5,\n",
    "                                  scoring=\"accuracy\",\n",
    "                                  return_train_score=True,\n",
    "                                  verbose=True,\n",
    "                                  n_jobs=-1)\n",
    "\n",
    "forest_grid_search.fit(X_train, y_train)\n",
    "\n",
    "forest_grid_search.best_params_\n",
    "\n",
    "forest_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Genetic Algorithms\n",
    "\n",
    "*Evolutionary algorithms* take their inspiration from biological evolution. \n",
    "\n",
    "Genetic algorithms are perhaps the most famous type of evolutionary algorithm.  They operate as follows:\n",
    "\n",
    "1. Generate an initial population of solutions\n",
    "2. Calculate the fitness of each individual in the population\n",
    "3. Select parents to produce offspring\n",
    "4. Carry out crossover and mutation to produce offspring for new population\n",
    " - Go to step 2 again and repeat until done\n",
    "\n",
    "We will discuss genetic algorithms in the context of *feature subset selection*, another important optimization problem in machine learning.\n",
    "- Easier to discuss using Microsoft Whiteboard\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
