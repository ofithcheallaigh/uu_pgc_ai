{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3.1 Optimization I\n",
    "\n",
    "By the end of this notebook you should be able to:\n",
    "\n",
    "1. Understand basic terminology of optimization\n",
    "2. Recognise the different optimization problems that arise in machine learning\n",
    "3. Describe a range of *loss functions* used in regression \n",
    "4. Understand the following optimization algorithms used to minimize cost functions:\n",
    "   - *Random Search*\n",
    "   - *Coordinate Search* \n",
    "   - *Gradient Descent*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1 Basic Terminology of Optimization\n",
    "\n",
    "An optimization problem involves finding the vector $\\hat{\\mathbf{x}}$ that minimizes some function $f(\\mathbf{x})$\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{x}} = \\text{arg min} f(\\mathbf{x})\n",
    "\\end{equation}\n",
    "\n",
    "### 1.1 The design vector $\\mathbf{x}$\n",
    "\n",
    "- The vector $\\mathbf{x}$ is called the *design vector*\n",
    "- Often, $\\mathbf{x}$ will be a vector of real valued design variables:\n",
    "  - In *bounded* optimization problems, the real-valued design variables have lower and upper bounds $l_i \\le x_i \\le u_i$.\n",
    "  - In *unbounded* problems, the real-valued design variables have no bounds $-\\infty \\le x_i \\le \\infty$.\n",
    "- However some components of $\\mathbf{x}$ may be integer or categorical, depending on the problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.2 The objective function $f(\\mathbf{x})$\n",
    "\n",
    "- The function $f$ is called the *objective function*\n",
    "- Sometimes the problem is to maximize $f(\\mathbf{x})$; but this is equivalent to minimizing  $-f(\\mathbf{x})$\n",
    "- Sometimes, we will know the first derivatives of $f$, and possibly the second derivatives too\n",
    " - This is an important consideration when choosing an optimization algorithm to optimize $f$\n",
    "- Evaluating $f$ may be very computationally expensive (e.g. days to compute), or it may be cheap (< 1s)\n",
    "  - This is also an important consideration when choosing an optimization algorithm to optimize $f$\n",
    "- Sometimes we may have more than one objective, i.e. we may need to minimize $f_i(\\mathbf{x}), i = 1,2,\\ldots N$. \n",
    "  - This is called *multi-objective optimization*. It is not considered much in ML text-books, but is very applicable to ML and is a useful technique to know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.3 The solution $\\hat{\\mathbf{x}}$\n",
    "\n",
    "- We generally want to find the **global** minimum of $f$:   \n",
    "  - This is the $\\hat{\\mathbf{x}}$ such that $f(\\hat{\\mathbf{x}}) \\le f(\\mathbf{x})$ for all $\\mathbf{x}$ in our search space\n",
    "- There may be multiple global minima for $f$, or there may be just one\n",
    "  - If there are multiple, then ideally we'd like our algoritm to return them all\n",
    "- $f$ may have also one or more **local** minima:\n",
    "  - A point $\\mathbf{x}^{*}$ is a local minima if $f(\\mathbf{x}^{*}) \\le f(\\mathbf{x})$ for all $\\mathbf{x}$ near $\\mathbf{x}^{*}$\n",
    "  - The meaning of *near* is relative: it just means within some neighbourhood around $\\mathbf{x}^{*}$\n",
    "  \n",
    "- When we have multiple objectives, things become a bit different:\n",
    "  - Generally the objectives 'compete' and we are interested in finding a set of optimal solutions called the *Pareto optimal set* (that represents the best possible trade-off in objectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![MLR2_01](Images/MLR_Fig2_01.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Optimization problems in machine learning\n",
    "\n",
    "There are a number of different optimization problems that arise in machine learning, e.g.:\n",
    "\n",
    "1. The optimization of model *parameters*, e.g. the weights in linear regression\n",
    "2. The optimization of model *hyperparameters*, e.g. the choice of $\\lambda$ in regularization, the choice of degree in polynomial regression\n",
    "3. The optimal selection of a subset of features to train from\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Optimization of model parameters\n",
    "\n",
    "The model that we have focused on so far is the linear regression model:\n",
    "  - Linear model $f(\\mathbf{x}) = w_0 + \\sum_{i=1}^d w_ix_i $\n",
    "  - Or more generally: $f(\\mathbf{x}) = w_0 + \\sum_{i=1}^d w_i\\phi(x_i) $ where $\\phi()$ is a function used to achieve non-linearity (e.g. polynomials, sin function used last week).\n",
    "\n",
    "A linear model has parameters $w_i$ that controls its behaviour.\n",
    "\n",
    "During *training*, we search for the weight vector $\\hat{\\mathbf{w}}$ that minimizes a *cost function* $J(\\mathbf{w})$.\n",
    " - the weight vector $\\mathbf{w}$ is our design vector. Often $\\mathbf{\\theta}$ is usually used to represent the parameters of a general model, but we will continue with $\\mathbf{w}$.\n",
    " - the cost function $J$ is our objective function to be minimized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Empirical risk minimization\n",
    "\n",
    "Typically $J$ is defined as an average *loss* over the training set:\n",
    "\n",
    "\\begin{equation} \n",
    "J (\\mathbf{w}) = \\dfrac{1}{N} \\sum_{i=1}^{N} L(f(\\mathbf{x}_i; \\mathbf{w}), y_i)\n",
    "\\end{equation}\n",
    "\n",
    "where:\n",
    " - $N$ is the number of examples in the training set\n",
    " - $L$ is known as a per-example *loss function*; it usually represents the residual\n",
    " \n",
    "Training a model using a cost function of this form is known as *empirical risk minimization*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3 Loss functions used in regression\n",
    "\n",
    "### Squared Error (L2 Loss)\n",
    "\n",
    "We have already seen the mean squared error cost function\n",
    "\n",
    "\\begin{equation} \n",
    "J (\\mathbf{w}) = \\dfrac{1}{N} \\sum_{i=1}^{N} (y_i - f(\\mathbf{x}_i; \\mathbf{w}))^2\n",
    "\\end{equation}\n",
    "\n",
    "which uses squared error as the loss: \\begin{equation} L(f(\\mathbf{x}_i; \\mathbf{w}), y_i) = (y_i - f(\\mathbf{x}_i; \\mathbf{w}))^2 \\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Susceptibility of L2 Loss to outliers\n",
    "\n",
    "![MLR5_07](Images/MLR_Fig5_06_notes.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Absolute Error (L1 Loss)\n",
    "\n",
    "\\begin{equation} L(f(\\mathbf{x}_i; \\mathbf{w}), y_i) = |(y_i - f(\\mathbf{x}_i; \\mathbf{w}))| \\end{equation}\n",
    "\n",
    "- Gives rise to the Mean Absolute Deviations (MAD) cost function\n",
    "\n",
    "\\begin{equation} \n",
    "J (\\mathbf{w}) = \\dfrac{1}{N} \\sum_{i=1}^{N} |(y_i - f(\\mathbf{x}_i; \\mathbf{w}))|\n",
    "\\end{equation}\n",
    "\n",
    "- L1 loss is more robust to outliers in the training data as it does not place such emphasis on reducing individual large errors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![MLR5_07](Images/MLR_Fig5_07.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The L1 loss function poses some difficulties for optimization algorithms:\n",
    " - its gradient with respect to error (residual) does not really provide much information to optimization algorithms, as it is constant (it is equal to either -1 or 1)\n",
    " - it is not differentiable at the point where the error is zero\n",
    "\n",
    "Two loss functions which are robust to outliers, and overcome these limitations of L1 (to different degrees) are the Huber Loss and the Log-Cosh.  \n",
    "\n",
    "One of your lab exercises today is to use Huber Loss in scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Huber Loss\n",
    "\n",
    "\\begin{equation}\n",
    "  L(f(\\mathbf{x}_i; \\mathbf{w}), y_i) =\\left\\{\n",
    "  \\begin{array}{@{}ll@{}}\n",
    "    \\dfrac{1}{2}(y_i - f(\\mathbf{x}_i; \\mathbf{w}))^2 & \\text{for}\\ |(y_i - f(\\mathbf{x}_i; \\mathbf{w}))| \\le \\delta \\\\\n",
    "    \\delta |(y_i - f(\\mathbf{x}_i; \\mathbf{w}))|  - \\dfrac{1}{2}\\delta^2, & \\text{otherwise}\n",
    "  \\end{array}\\right.\n",
    "\\end{equation} \n",
    "\n",
    "where $\\delta$ is a hyperparameter which can be tuned.\n",
    "\n",
    "### Log-Cosh Loss\n",
    "\n",
    "\\begin{equation} L(f(\\mathbf{x}_i; \\mathbf{w}), y_i) = \\log(\\cosh (y_i - f(\\mathbf{x}_i; \\mathbf{w}))) \\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4 Optimization algorithms for determining optimal weights\n",
    "\n",
    "There are many, many optimization algorithms in use for determining optimal model weights (training)\n",
    "- Some recent review papers are listed in the final lab exercise today for you to check out\n",
    " \n",
    "We will look at some of the simplest because:\n",
    "- They are commonly used\n",
    "- Many of the more sophisticated algorithms build on top of them\n",
    "- We are limited in time in what we can do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## General idea of local optimization methods\n",
    "\n",
    "![MLR2_04](Images/MLR_Fig2_04_notes.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The general framework\n",
    "\n",
    "- Start with an intial point $\\mathbf{w}^0$\n",
    "- From $\\mathbf{w}^0$, determine a *descent direction* $\\mathbf{d}^0$ in our design space, to take us to our next point $\\mathbf{w}^1$ with lower objective function value, i.e. $g(\\mathbf{w}^1)  < g(\\mathbf{w}^0)$\n",
    "  - $\\mathbf{w}^1 = \\mathbf{w}^0 + \\mathbf{d}^0$\n",
    "- From $\\mathbf{w}^1$, determine a *descent direction* $\\mathbf{d}^1$ in our design space, to take us to our next point $\\mathbf{w}^2$ with lower objective function value, i.e. $g(\\mathbf{w}^2)  < g(\\mathbf{w}^1)$  \n",
    "- Keep repeating this process: at the $k$-th iteration we are moving to point $\\mathbf{w}^k = \\mathbf{w}^{k-1} + \\mathbf{d}^{k-1}$\n",
    "- Descent directions can be determined in a variety of ways, with or without derivatives\n",
    "  - This is what distinguishes different local optimization algorithms from one another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How far to travel in the descent direction?\n",
    "\n",
    "![MLR2_05](Images/MLR_Fig2_05_notes.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The steplength parameter\n",
    "\n",
    "- Because of this potential problem, many local optimization algorithms have something called a *steplength parameter*\n",
    "  - This is also referred to as *learning rate* in many algorithms\n",
    "- The steplength parameter, denoted $\\alpha$, allows us to control the length of each update step as follows:\n",
    "\n",
    "\\begin{equation} \\mathbf{w}^k = \\mathbf{w}^{k-1} + \\alpha\\mathbf{d}^{k-1}\\end{equation}\n",
    "\n",
    "- Using the steplength parameter, we can fine tune precisely how far we want to travel in the descent direction at each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Random Search\n",
    "\n",
    "Random search chooses the descent direction from $\\mathbf{w}^{k-1}$ by: \n",
    "- Generating a number $P$ of random directions $\\{\\mathbf{d}\\}^{P}_{p=1}$ to try out from current point $\\mathbf{w}^{k-1}$\n",
    "- This gives rise to candidate points $\\mathbf{w}^{k-1} + \\mathbf{d}^p$ for $p=1,2,\\ldots,P$\n",
    "- Evaluate all of these $P$ candidate points, and find the best (i.e. the one smallest objective function value)\n",
    "- Suppose the best is $\\mathbf{w}^{k-1} + \\mathbf{d}^s$\n",
    "  - If $g(\\mathbf{w}^{k-1} + \\mathbf{d}^s) < g(\\mathbf{w}^{k-1})$ then we move to $\\mathbf{w}^k = \\mathbf{w}^{k-1} + \\mathbf{d}^s$\n",
    "  - Otherwise we halt the method, or try another batch of $P$ random directions.\n",
    "\n",
    "#### Controlling the length of each step\n",
    "- When generating the $P$ random directions $\\{\\mathbf{d}\\}^{P}_{p=1}$, these are usually normalized (to have length 1)\n",
    "- Then, the candidate points above become $\\mathbf{w}^k = \\mathbf{w}^{k-1} + \\alpha\\mathbf{d}^p$ for $p=1,2,\\ldots,P$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![MLR2_06](Images/MLR_Fig2_06.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![MLR2_07](Images/MLR_Fig2_07side.jpg)\n",
    "\n",
    "- Visualisation of random search with 5 iterations and $\\alpha = 1$ for all steps\n",
    "- Initial point is in green; final point is in red\n",
    "- The right hand plot shows a cost function history plot - useful for higher dimensional problems when we can't plot the surface or contour plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![MLR2_08](Images/MLR_Fig2_08.jpg)\n",
    "\n",
    "We can attempt *global* optimization using random search (or any other local optimization algorithm), by carying out multiple runs of the algorithm using different initializations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Failure to converge\n",
    "\n",
    "![MLR2_09](Images/MLR_Fig2_09.jpg)\n",
    "\n",
    "- Left hand figure: using a different starting point to before we arrive at the final sub-optimal point shown in red, for which every direction is ascent, so the algorithm halts.\n",
    "- Middle figure: If we cut the steplength down to $\\alpha = 0.01$, then it takes a long time to move anywhere\n",
    "- Right hand figure: We have a steplength of $\\alpha=0.1$ for all steps; this converges to a point close to the optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Diminishing steplength rules\n",
    "\n",
    "- An alternative to using a fixed steplength throughout is to use a *diminishing steplength rule*\n",
    "- Such a rule shrinks the steplength at each step of local optimization\n",
    "- For example, a simple rule to shrink the stepength at iteration $k$ would be to set $\\alpha = \\frac{1}{k}$\n",
    "\n",
    "**Part of your coursework will be to implement a diminishing steplength rule in a random search algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coordinate search and coordinate descent\n",
    "\n",
    "![MLR2_11](Images/MLR_Fig2_11.jpg)\n",
    "\n",
    "- Left-hand figure: **Coordinate search** is like random search, except the descent directions are constrained to be parallel to the coordinate axes of the design space. Therefore, if design space has dimension $N$, then there are only $2N$ directions to consider at each iteration.\n",
    "- Right-hand figure: **Coordinate descent** is similar, except at each iteration we simply examine one coordinate direction (and its negative) and step in this direction if it produces descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Consider a 1d function $f(x)$, with derivative $f'(x)$. Then $f'(x)$ gives the slope of $f(x)$ at $x$.\n",
    "\n",
    "Consider an initial point $x^*$, and calculate $f(x^*)$ and $f'(x^*)$:\n",
    "- If $f'(x^*)$ is positive, then:\n",
    "  - if we increase $x$ slightly (from $x^*$), then $f(x)$ will increase (from $f(x^*)$)\n",
    "  - if we decrease $x$ slightly (from $x^*$), then $f(x)$ will decrease (from $f(x^*)$)\n",
    "- If $f'(x^*)$ is negative, then: \n",
    "  - if we increase $x$ slightly (from $x^*$), then $f(x)$ will decrease (from $f(x^*)$) \n",
    "  - if we decrease $x$ slightly (from $x^*$), then $f(x)$ will increase (from $f(x^*)$)\n",
    "\n",
    "So the first derivative is useful, because it tells us how to change $x$ in order to improve (decrease) $f(x)$\n",
    "\n",
    "In multi-dimensional problems, this is generalized to the negative gradient $- \\nabla g(\\mathbf{w})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![GradDesc](Images/grad_descent2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![MLR3_08](Images/MLR_Fig3_08_notes.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Steplength (also known as learning rate) is another example of a hyperparameter\n",
    "- That is, it is something that:\n",
    "  - is not optimized during the training of our machine learning model (i.e. by optimizing the cost function)\n",
    "  - but it is something that we should try to optimize ourselves\n",
    "  \n",
    "We will cover hyperparameter optimizaton next."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
