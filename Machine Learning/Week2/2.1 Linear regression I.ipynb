{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2.1 Linear regression\n",
    "\n",
    "By the end of this notebook you should be able to:\n",
    "\n",
    "- Use matrices to formulate linear regression when there is more than one feature (*multiple* linear regression)\n",
    "- Understand how to determine the optimal values of weights for multiple linear regression using:\n",
    "  - the normal equations\n",
    "  - an optimization algorithm\n",
    "- Understand a basic python implementation of multiple linear regression (using optimization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap of simple linear regression\n",
    "\n",
    "Last time we looked at *linear* regression where there is only *one* single feature $x$:\n",
    "\n",
    "\\begin{equation} f(x) = w_0 + w_1 x \\end{equation}\n",
    "\n",
    "- This is *simple* linear regression.\n",
    "- The model has only two parameters that we need fit: $w_0$ and $w_1$.\n",
    "- If we use the mean squared error $\\frac{1}{N}\\sum_{i=1}^N (y_i - (w_0 + w_1x_i))^2$ to fit the line, then we can differentiate this to determine the optimal values of $w_0$ and $w_1$ via the normal equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Beyond simple linear regression\n",
    "\n",
    "Simple linear regression is a useful starting point, but it is limiting in at least two ways:\n",
    "\n",
    "1. We may have multiple features $x_1, x_2, x_3, \\ldots, x_d$ for our problem, and we may want to include several (or all) of them in our model to predict the label $y$. This leads us to **multiple linear regression**.\n",
    "\n",
    "![FromSimpleToMulti](Images/MLR_Fig5_01.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "  2. Restricting ourselves to a linear function of the features may not be appropriate. For example:\n",
    "   - In the case where we only one single feature $x$, the model $f(x) = w_0 + w_1x$ (i.e. a straight line) will give a poor fit if the relationship between $x$ and $y$ is not linear\n",
    "   - More generally, when we have multiple features, we don't want to be restricted to linear relationships between $y$ and each feature $x_i$\n",
    "      \n",
    "![FromSimpleToPoly](Images/simple_to_poly2_cite.png)\n",
    "\n",
    "We will cover multiple linear regression first: extending our model to cover multiple features.\n",
    "\n",
    "In doing so, we will cover the mathematics necessary to consider non-linear responses from a linear model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple linear regression\n",
    "\n",
    "- Suppose each example in our data-set has $d$ features: $x_1, x_2, \\ldots , x_d$\n",
    "- We can represent the features of each example in our data-set as a vector. The features of the $i$-th example would be represented as \\begin{equation} \\mathbf{x}_i = \\begin{bmatrix} x_{i1} \\\\ x_{i2} \\\\ \\vdots \\\\ x_{id} \\end{bmatrix} \\end{equation}\n",
    "i.e. we use $x_{ij}$ to denote the $j$-th feature of example $i$\n",
    "  - Don't confuse this with notation $a_{ij}$ used last week for the element in the $i$-th row and $j$-th column of a matrix\n",
    "- Suppose we have a data-set of $N$ examples $\\{(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), \\ldots , (\\mathbf{x}_N, y_N)\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can construct a linear function of the $d$ features as follows:\n",
    "\n",
    "\\begin{align} f(\\mathbf{x}) & = w_0 + w_1x_1 + w_2x_2 + \\ldots + w_dx_d \\\\ & = w_0 + \\sum_{i=1}^d w_i x_i \\end{align}\n",
    "\n",
    "This can be written compactly as $f(x) = w_0 + \\mathbf{w}^T\\mathbf{x}$ where:\n",
    "\n",
    "the vector $\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix}$ represents the features, and the vector $\\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_d \\end{bmatrix}$ represents the *feature touching* weights. Weight $w_0$ does not touch a feature and is referred to as the *bias*. \n",
    "\n",
    "**We can make things even neater by including the bias inside the weight vector however.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose instead that the weight vector includes the bias term $w_0$, i.e. $\\mathbf{w} = \\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_d \\end{bmatrix}$\n",
    "\n",
    "And suppose we also augment the feature vector with a 1, i.e. $\\mathbf{x} = \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix}$\n",
    "\n",
    "Then $f(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} = w_0 + w_1x_1 + w_2x_2 + \\ldots + w_dx_d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Representing the least squares cost function in terms of matrices\n",
    "\n",
    "Recall that the least squares cost function is the mean of the sum of squared residuals in our model:\n",
    "\\begin{equation}g(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^N (y_i - f(\\mathbf{x}_i))^2\\end{equation}\n",
    "\n",
    "With our matrix formulation,  $f(\\mathbf{x}_i) = \\mathbf{w}^T\\mathbf{x}_i$ (assuming now the $\\mathbf{x}_i$ is augmented with a 1), so this cost function can be written as:\n",
    "\\begin{equation} g(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^N (y_i - \\mathbf{w}^T\\mathbf{x}_i)^2 \\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This can then be simplified further as\n",
    "\n",
    "\\begin{equation}g(\\mathbf{w}) = \\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{w})\\end{equation}\n",
    "\n",
    "where:\n",
    "\n",
    "$\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1^T \\\\ \\mathbf{x}_2^T \\\\ \\vdots \\\\ \\mathbf{x}_N^T \\end{bmatrix} = \\begin{bmatrix} 1 & x_{11} & x_{12} & x_{13} & \\ldots & x_{1d} \\\\ 1 & x_{21} & x_{22} & x_{23} & \\ldots & x_{2d}\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{N1} & x_{N2} & x_{N3} & \\ldots & x_{Nd} \\end{bmatrix}$ \n",
    "\n",
    "and $\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similar to before, we can differentiate $g(\\mathbf{w}) = \\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{w})$ with respect to $\\mathbf{w}$, and derive normal equations for the case of multiple linear regression. Doing so gives us:\n",
    "\n",
    "\\begin{equation} \\hat{\\mathbf{w}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\end{equation}\n",
    "\n",
    "**It is part of a coursework question to implement this.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Limitations of Normal equations\n",
    "\n",
    "- Using the normal equations involves computing $(\\mathbf{X}^T\\mathbf{X})^{-1}$\n",
    "- This is a $d \\times d$ matrix, where $d$ is the number of features.\n",
    "- The computational complexity of inverting a $d \\times d$ matrix is roughly $d^3$, although algorithms exist that take this down to about $d^{2.4}$\n",
    "- Therefore, if number of feaures $d$ increases by a factor of 10, then the computational complexity increases by a factor of $10^{2.4} \\approx 250$\n",
    "- For large enough $d$, the cost becomes prohibitive, and using the normal equations isn't feasible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Optimization\n",
    "\n",
    "- Using normal equations is not normal: most machine learning algorithms don't have closed-form solutions for their optimal parameter values.\n",
    "- Optimization algorithms are almost always used to determine the best model parameters for a machine learning algorithm. We will look at optimization algorithms in more depth next week.\n",
    "- For this week, we will just use a random search optimization algorithm as a black box, and look in more detail at how it works next week.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Implementing the Least Squares model as an optimization problem in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1\n",
    "\n",
    "- Read in our data into matrices $\\mathbf{X}$ and $\\mathbf{y}$\n",
    "- Here we will just create some dummy data to test our code out as we go along\n",
    "\n",
    "**Better:** \n",
    "- Read in real data from a csv file into `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 2\n",
    "\n",
    "Now we need to add an extra column of 1s to `X`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 3\n",
    "\n",
    "- Create a function called `model()` that:\n",
    "  - takes as input a vector `x` and a weight vector `w`\n",
    "  - returns as output the linear combination $f(\\mathbf{x}) = w_0 + w_1x_1 + w_2x_2 + \\ldots + w_dx_d$\n",
    "- We need to be very careful as to whether `x` contains the added 1 or not:\n",
    "  - If it does: $f(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} = w_0 + w_1x_1 + w_2x_2 + \\ldots + w_dx_d$\n",
    "  - If it does not: $f(\\mathbf{x}) = w_0 + \\mathbf{w}_1^T \\mathbf{x} = w_0 + w_1x_1 + w_2x_2 + \\ldots + w_dx_d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 4\n",
    "\n",
    "Lets test our `model` function out with a dummy set of weights `w`, and a dummy input vector `x_new`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 5\n",
    "\n",
    "Write our cost function `least_squares` that calculates the mean squared error for our model using \n",
    "\n",
    "\\begin{equation}g(\\mathbf{w}) = \\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{w})\\end{equation}\n",
    "\n",
    "- The input to `least_squares` should be weights `w`\n",
    "    - It will also use matrix `X` and vector `y` (not passed in)\n",
    "- The function should return the value of `g(w)` as given by the formula above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 6\n",
    "\n",
    "Test out the `least_squares` function by calling it with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 7\n",
    "\n",
    "- In the cell below we will use the *random search* algorithm for optimization.\n",
    "- Optimization will be covered next week - for this week we treat this function as a 'black box' that:\n",
    "  - takes a cost function as an input `g`\n",
    "  - takes an initial weight vector `w`\n",
    "  - other input parameters will be explained next week\n",
    "  - and returns the history of the search for optimal parameters values (using the cost function we specified) as output\n",
    "    - last value will be the best set of parameter values found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def random_search(g,alpha_choice,max_its,w,num_samples):\n",
    "    # run random search\n",
    "    weight_history = []         # container for weight history\n",
    "    cost_history = []           # container for corresponding cost function history\n",
    "    alpha = 0\n",
    "    for k in range(1,max_its+1):        \n",
    "        # check if diminishing steplength rule used\n",
    "        if alpha_choice == 'diminishing':\n",
    "            alpha = 1/float(k)\n",
    "        else:\n",
    "            alpha = alpha_choice\n",
    "            \n",
    "        # record weights and cost evaluation\n",
    "        weight_history.append(w.T)\n",
    "        cost_history.append(g(w.T))\n",
    "        \n",
    "        # construct set of random unit directions\n",
    "        directions = np.random.randn(num_samples,np.size(w))\n",
    "        norms = np.sqrt(np.sum(directions*directions,axis = 1))[:,np.newaxis]\n",
    "        directions = directions/norms   \n",
    "        \n",
    "        ### pick best descent direction\n",
    "        # compute all new candidate points\n",
    "        w_candidates = w + alpha*directions\n",
    "        \n",
    "        # evaluate all candidates\n",
    "        evals = np.array([g(w_val.T) for w_val in w_candidates])\n",
    "\n",
    "        # if we find a real descent direction take the step in its direction\n",
    "        ind = np.argmin(evals)\n",
    "        if g(w_candidates[ind]) < g(w.T):\n",
    "            # pluck out best descent direction\n",
    "            d = directions[ind,:]\n",
    "        \n",
    "            # take step\n",
    "            w = w + alpha*d\n",
    "        \n",
    "    # record weights and cost evaluation\n",
    "    weight_history.append(w.T)\n",
    "    cost_history.append(g(w.T))\n",
    "    return weight_history,cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"1000\" height=\"400\" controls loop>\n",
       "  <source src=\"Videos/animation_2.mp4\" type=\"video/mp4\">\n",
       "  </video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Video below from https://github.com/jermwatt/machine_learning_refined\n",
    "# Shared under the Creative Commons Attribution 4.0 International License (CC BY-NC-SA 4.0)\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"1000\" height=\"400\" controls loop>\n",
    "  <source src=\"Videos/animation_2.mp4\" type=\"video/mp4\">\n",
    "  </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example of multiple linear regression\n",
    "\n",
    "1. The Wine data-set that you looked at in labs last week contained multiple features, which could be used to predict the label of *quality*. **This will be one of the lab exercises today**.\n",
    "\n",
    "\n",
    "2. Using the example from *First Course in Machine Learning*, of winning times in the men's 100m Olympics finals, we could have three features as follows:\n",
    "  - $x_1 = $ year\n",
    "  - $x_2 =$ wind speed at time of the race\n",
    "  - $x_3 = $ best time run so far by any competitor that year\n",
    "\n",
    "  At the minute, we only have values for $x_1$ in our data-set for Olympics.  \n",
    "  - We could go search the internet for the values of $x_2$ and $x_3$ for each year\n",
    "  - Or we could construct new features from the feature we currently have, i.e. $x_1$ (polynomial regression - next lecture)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
