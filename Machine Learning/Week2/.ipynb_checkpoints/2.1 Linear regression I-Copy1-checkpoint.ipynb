{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2.1 Linear regression\n",
    "\n",
    "By the end of this notebook you should be able to:\n",
    "\n",
    "- Use matrices to formulate linear regression when there is more than one feature (*multiple* linear regression)\n",
    "- Understand how to determine the optimal values of weights for multiple linear regression using:\n",
    "  - the normal equations\n",
    "  - an optimization algorithm\n",
    "- Understand a basic python implementation of multiple linear regression (using optimization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap of simple linear regression\n",
    "\n",
    "Last time we looked at *linear* regression where there is only *one* single feature $x$:\n",
    "\n",
    "\\begin{equation} f(x) = w_0 + w_1 x \\end{equation}\n",
    "\n",
    "- This is *simple* linear regression.\n",
    "- The model has only two parameters that we need fit: $w_0$ and $w_1$.\n",
    "- If we use the mean squared error $\\frac{1}{N}\\sum_{i=1}^N (y_i - (w_0 + w_1x_i))^2$ to fit the line, then we can differentiate this to determine the optimal values of $w_0$ and $w_1$ via the normal equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Beyond simple linear regression\n",
    "\n",
    "Simple linear regression is a useful starting point, but it is limiting in at least two ways:\n",
    "\n",
    "1. We may have multiple features $x_1, x_2, x_3, \\ldots, x_d$ for our problem, and we may want to include several (or all) of them in our model to predict the label $y$. This leads us to **multiple linear regression**.\n",
    "\n",
    "![FromSimpleToMulti](Images/MLR_Fig5_01.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "  2. Restricting ourselves to a linear function of the features may not be appropriate. For example:\n",
    "   - In the case where we only one single feature $x$, the model $f(x) = w_0 + w_1x$ (i.e. a straight line) will give a poor fit if the relationship between $x$ and $y$ is not linear\n",
    "   - More generally, when we have multiple features, we don't want to be restricted to linear relationships between $y$ and each feature $x_i$\n",
    "      \n",
    "![FromSimpleToPoly](Images/simple_to_poly2.png)\n",
    "\n",
    "We will cover multiple linear regression first: extending our model to cover multiple features.\n",
    "\n",
    "In doing so, we will cover the mathematics necessary to consider non-linear responses from a linear model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple linear regression\n",
    "\n",
    "- Suppose each example in our data-set has $d$ features: $x_1, x_2, \\ldots , x_d$\n",
    "- We can represent the features of each example in our data-set as a vector. The features of the $i$-th example would be represented as \\begin{equation} \\mathbf{x}_i = \\begin{bmatrix} x_{i1} \\\\ x_{i2} \\\\ \\vdots \\\\ x_{id} \\end{bmatrix} \\end{equation}\n",
    "i.e. we use $x_{ij}$ to denote the $j$-th feature of example $i$\n",
    "  - Don't confuse this with notation $a_{ij}$ used last week for the element in the $i$-th row and $j$-th column of a matrix\n",
    "- Suppose we have a data-set of $N$ examples $\\{(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), \\ldots , (\\mathbf{x}_N, y_N)\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can construct a linear function of the $d$ features as follows:\n",
    "\n",
    "\\begin{align} f(\\mathbf{x}) & = w_0 + w_1x_1 + w_2x_2 + \\ldots + w_dx_d \\\\ & = w_0 + \\sum_{i=1}^d w_i x_i \\end{align}\n",
    "\n",
    "This can be written compactly as $f(x) = w_0 + \\mathbf{w}^T\\mathbf{x}$ where:\n",
    "\n",
    "the vector $\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix}$ represents the features, and the vector $\\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_d \\end{bmatrix}$ represents the *feature touching* weights. Weight $w_0$ does not touch a feature and is referred to as the *bias*. \n",
    "\n",
    "**We can make things even neater by including the bias inside the weight vector however.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose instead that the weight vector includes the bias term $w_0$, i.e. $\\mathbf{w} = \\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_d \\end{bmatrix}$\n",
    "\n",
    "And suppose we also augment the feature vector with a 1, i.e. $\\mathbf{x} = \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix}$\n",
    "\n",
    "Then $f(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} = w_0 + w_1x_1 + w_2x_2 + \\ldots + w_dx_d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Representing the least squares cost function in terms of matrices\n",
    "\n",
    "Recall that the least squares cost function is the mean of the sum of squared residuals in our model:\n",
    "\\begin{equation}g(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^N (y_i - f(\\mathbf{x}_i))^2\\end{equation}\n",
    "\n",
    "With our matrix formulation,  $f(\\mathbf{x}_i) = \\mathbf{w}^T\\mathbf{x}_i$, so this cost function can be written as:\n",
    "\\begin{equation} g(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^N (y_i - \\mathbf{w}^T\\mathbf{x}_i)^2 \\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This can then be simplified further as\n",
    "\n",
    "\\begin{equation}g(\\mathbf{w}) = \\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{w})\\end{equation}\n",
    "\n",
    "where:\n",
    "\n",
    "$\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1^T \\\\ \\mathbf{x}_2^T \\\\ \\vdots \\\\ \\mathbf{x}_N^T \\end{bmatrix} = \\begin{bmatrix} 1 & x_{11} & x_{12} & x_{13} & \\ldots & x_{1d} \\\\ 1 & x_{21} & x_{22} & x_{23} & \\ldots & x_{2d}\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{N1} & x_{N2} & x_{N3} & \\ldots & x_{Nd} \\end{bmatrix}$ \n",
    "\n",
    "and $\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Proof in Whiteboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similar to before, we can differentiate $g(\\mathbf{w}) = \\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{w})$ with respect to $\\mathbf{w}$, and derive normal equations for the case of multiple linear regression. Doing so gives us:\n",
    "\n",
    "\\begin{equation} \\hat{\\mathbf{w}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\end{equation}\n",
    "\n",
    "**It is part of a coursework question to implement this.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Limitations of Normal equations\n",
    "\n",
    "- Using the normal equations involves computing $(\\mathbf{X}^T\\mathbf{X})^{-1}$\n",
    "- This is a $d \\times d$ matrix, where $d$ is the number of features.\n",
    "- The computational complexity of inverting a $d \\times d$ matrix is roughly $d^3$, although algorithms exist that take this down to about $d^{2.4}$\n",
    "- Therefore, if number of feaures $d$ increases by a factor of 10, then the computational complexity increases by a factor of $10^{2.4} \\approx 250$\n",
    "- For large enough $d$, the cost becomes prohibitive, and using the normal equations isn't feasible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Optimization\n",
    "\n",
    "- Using normal equations is not normal: most machine learning algorithms don't have closed-form solutions for their optimal parameter values.\n",
    "- Optimization algorithms are almost always used to determine the best model parameters for a machine learning algorithm. We will look at optimization algorithms in more depth next week.\n",
    "- For this week, we will just use a random search optimization algorithm as a black box, and look at it in more detail at how it works next week.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Implementing the Least Squares model as an optimization problem in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1\n",
    "\n",
    "- Read in our data into matrices $\\mathbf{X}$ and $\\mathbf{y}$\n",
    "- Here we will just create some dummy data to test our code out as we go along\n",
    "  - We deliberately use simple dummy data, so we can check calculations by hand as we go along\n",
    "\n",
    "\n",
    "**Lab:** \n",
    "- Read in real data from a csv file into `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2 0.3]\n",
      " [0.4 0.2]\n",
      " [0.8 0.5]\n",
      " [0.6 0.3]]\n",
      "[[1]\n",
      " [2]\n",
      " [2]\n",
      " [4]]\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[0.2,0.3],[0.4,0.2],[0.8,0.5],[0.6,0.3]])\n",
    "y = np.array([[1],[2],[2],[4]])\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "print(X[0,:].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 2\n",
    "\n",
    "Now we need to add an extra column of 1s to `X`\n",
    "\n",
    "**Lab**: \n",
    "- remove hard-coding of shape passed to `np.ones()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  0.2 0.3]\n",
      " [1.  0.4 0.2]\n",
      " [1.  0.8 0.5]\n",
      " [1.  0.6 0.3]]\n"
     ]
    }
   ],
   "source": [
    "X_with_ones = np.hstack((np.ones((4,1)),X))\n",
    "print(X_with_ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 3\n",
    "\n",
    "- Create a function called `model()` that:\n",
    "  - takes as input a vector `x` and a weight vector `w`\n",
    "  - returns as output the linear combination $f(\\mathbf{x}) = w_0 + w_1x_1 + w_2x_2 + \\ldots + w_dx_d$\n",
    "- We need to be very careful as to whether `x` contains the added 1 or not:\n",
    "  - If it does: $f(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} = w_0 + w_1x_1 + w_2x_2 + \\ldots + w_dx_d$\n",
    "  - If it does not: $f(\\mathbf{x}) = w_0 + \\mathbf{w}_1^T \\mathbf{x} = w_0 + w_1x_1 + w_2x_2 + \\ldots + w_dx_d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def model(x,w):\n",
    "    print('x.shape =', x.shape)\n",
    "    print('w.shape =', w.shape)\n",
    "    print('w[1:,].T.shape =', w[1:,].T.shape)\n",
    "    inter = np.matmul(w[1:,].T,x)\n",
    "    print(\"inter.shape = \",inter.shape)\n",
    "    print(\"inter[0,0] = \",inter[0,0])\n",
    "    print(\"w[0,0] =\",w[0,0])\n",
    "    val = w[0,0] + np.matmul(w[1:,].T,x)[0,0]\n",
    "    print('val.shape =', val.shape)\n",
    "    return val\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 4\n",
    "\n",
    "Lets test our `model` function out with a dummy set of weights `w`, and a dummy input vector `x_new`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[1,2,3]]).T\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "print(w[1:,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(w[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (2, 1)\n",
      "w.shape = (3, 1)\n",
      "w[1:,].T.shape = (1, 2)\n",
      "inter.shape =  (1, 1)\n",
      "inter[0,0] =  2.1\n",
      "w[0,0] = 1\n",
      "val.shape = ()\n",
      "3.1\n"
     ]
    }
   ],
   "source": [
    "x_new = np.array([[0.3,0.5]]).T\n",
    "pred = model(x_new,w)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 5\n",
    "\n",
    "Write our cost function `least_squares` that calculates the mean squared error for our model using \n",
    "\n",
    "\\begin{equation}g(\\mathbf{w}) = \\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{w})\\end{equation}\n",
    "\n",
    "- The input to `least_squares` should be weights `w`\n",
    "    - It will also use matrix `X` and vector `y` (not passed in)\n",
    "- The function should return the value of `g(w)` as given by the formula above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def least_squares(w):\n",
    "    #print('X.shape = ',X_with_ones.shape)\n",
    "    #print('w.shape = ',w.shape)\n",
    "    #print('y.shape = ',y.shape)\n",
    "    Xw = np.matmul(X_with_ones,w)\n",
    "    y_minus_Xw = y - Xw\n",
    "    cost = np.matmul(y_minus_Xw.T,y_minus_Xw)/len(y)\n",
    "    return cost[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 6\n",
    "\n",
    "Test out the `least_squares` function by calling it with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7674999999999994\n"
     ]
    }
   ],
   "source": [
    "mse = least_squares(w)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 7\n",
    "\n",
    "- In the cell below we will use the *random search* algorithm for optimization.\n",
    "- Optimization will be covered next week - for this week we treat this function as a 'black box' that:\n",
    "  - takes a cost function as an input `g`\n",
    "  - takes an initial weight vector `w`\n",
    "  - other input parameters will be explained next week\n",
    "  - and returns the best set of parameters values for that cost function as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def random_search(g,alpha_choice,max_its,w,num_samples):\n",
    "    # run random search\n",
    "    weight_history = []         # container for weight history\n",
    "    cost_history = []           # container for corresponding cost function history\n",
    "    alpha = 0\n",
    "    for k in range(1,max_its+1):        \n",
    "        # check if diminishing steplength rule used\n",
    "        if alpha_choice == 'diminishing':\n",
    "            alpha = 1/float(k)\n",
    "        else:\n",
    "            alpha = alpha_choice\n",
    "            \n",
    "        # record weights and cost evaluation\n",
    "        weight_history.append(w.T)\n",
    "        cost_history.append(g(w.T))\n",
    "        \n",
    "        # construct set of random unit directions\n",
    "        directions = np.random.randn(num_samples,np.size(w))\n",
    "        norms = np.sqrt(np.sum(directions*directions,axis = 1))[:,np.newaxis]\n",
    "        directions = directions/norms   \n",
    "        \n",
    "        ### pick best descent direction\n",
    "        # compute all new candidate points\n",
    "        w_candidates = w + alpha*directions\n",
    "        \n",
    "        # evaluate all candidates\n",
    "        evals = np.array([g(w_val.T) for w_val in w_candidates])\n",
    "\n",
    "        # if we find a real descent direction take the step in its direction\n",
    "        ind = np.argmin(evals)\n",
    "        if g(w_candidates[ind]) < g(w.T):\n",
    "            # pluck out best descent direction\n",
    "            d = directions[ind,:]\n",
    "        \n",
    "            # take step\n",
    "            w = w + alpha*d\n",
    "        \n",
    "    # record weights and cost evaluation\n",
    "    weight_history.append(w.T)\n",
    "    cost_history.append(g(w.T))\n",
    "    return weight_history,cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh,sh1 = random_search(least_squares,alpha_choice='diminishing',max_its=100,w=np.array([[-2,-2,-2]]),num_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xiter = np.arange(0,101)\n",
    "plt.plot(xiter[0:], sh1[0:], label='constant')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('cost function')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best weight vector found =  [[ 2.65279503]\n",
      " [-1.05758732]\n",
      " [-0.63759473]]\n"
     ]
    }
   ],
   "source": [
    "print('best weight vector found = ',wh[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cost function value =  1.6393133840997802\n"
     ]
    }
   ],
   "source": [
    "print('best cost function value = ',sh1[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example of multiple linear regression\n",
    "\n",
    "1. The Wine data-set that you looked at in labs last week contained multiple features, which could be used to predict the label of *quality*. **This will be one of the lab exercises today**.\n",
    "\n",
    "\n",
    "2. Using the example from *First Course in Machine Learning*, of winning times in the men's 100m Olympics finals, we could have three features as follows:\n",
    "  - $x_1 = $ year\n",
    "  - $x_2 =$ wind speed at time of the race\n",
    "  - $x_3 = $ best time run so far by any competitor that year\n",
    "\n",
    "  At the minute, we only have values for $x_1$ in our data-set for Olympics.  \n",
    "  - We could go search the internet for the values of $x_2$ and $x_3$ for each year\n",
    "  - Or we could construct new features from the feature we currently have, i.e. $x_1$ (polynomial regerssion - next lecture)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
