{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2.2 Linear regression\n",
    "\n",
    "By the end of this notebook you should be able to:\n",
    "\n",
    "- Model a *nonlinear* response with a linear model (e.g. polynomial regression)\n",
    "- Understand the idea of *overfitting* and *underfitting*\n",
    "- Understand how to carry out *cross-validation*\n",
    "- Understand what *regularized* least squares regression is\n",
    "- Understand how weights in linear regression can be used to remove features (feature selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nonlinear response (with a linear model)\n",
    "\n",
    "The second restriction of simple linear regression we mentioned earlier was: *Restricting ourselves to a linear function of the features may not be appropriate.*\n",
    "\n",
    "To simplify discussion, suppose we still only have one feature $x$, and that the relationship between $x$ and $y$ looks quadratic.\n",
    " \n",
    "In this case, we would want to capture this relationship by adding an extra term $x^2$ to our model:\n",
    "\n",
    "\\begin{equation} f(x) = w_0 + w_1x + w_2x^2 \\end{equation}\n",
    "\n",
    "More generally however, we may want to add polynomial terms up to order $M$:\n",
    "\n",
    "\\begin{equation} f(x) = \\sum_{i=0}^{M} w_i x^i = w_0 + w_1x + w_2x^2 + \\ldots w_Mx^M \\end{equation}\n",
    "\n",
    "In this case, we can view the $x^i$ terms as additional features, and therefore the problem becomes one of multiple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For a $M$-th order polynomial, the design matrix becomes:\n",
    "\n",
    "\\begin{equation} \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1^T \\\\ \\mathbf{x}_2^T \\\\ \\vdots \\\\ \\mathbf{x}_N^T \\end{bmatrix} = \\begin{bmatrix} 1 & x_{1} & x_{1}^2 & x_{1}^3 & \\ldots & x_{1}^M \\\\ 1 & x_{2} & x_{2}^2 & x_{2}^3 & \\ldots & x_{2}^M\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{N} & x_{N}^2 & x_{N}^3 & \\ldots & x_{N}^M \\end{bmatrix} \\end{equation}\n",
    "\n",
    "where here $x_i^j$ means feature $x$ from example $i$ is raised to the power of $j$\n",
    "\n",
    "This is called the **Vandermonde matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![FromSimpleToPoly](Images/simple_to_poly2_cite.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have already covered everything we need to implement polynomial regression:\n",
    "- The only new step now is to construct the Vandermonde matrix from our single feature $x$ \n",
    "  - rather than a matrix containing completely different features ($x_1, x_2, \\ldots$) in each column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.loadtxt('Data/olympic100m.txt',delimiter=',')\n",
    "x = data[:,0][:,None]\n",
    "t = data[:,1][:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "##scale the data\n",
    "x = (x-1896)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "maxorder = 8\n",
    "X = np.ones_like(x) # This constructs a matrix the same shape as x, but full of 1s\n",
    "for i in range(1,maxorder+1):\n",
    "    X = np.hstack((X,x**i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note the terminology regarding *linear* can be confusing.\n",
    " \n",
    "The example of simple linear regression was linear in both the model parameters *and* the features. \n",
    " - It is tempting to think $y = w_0 + w_1 x$ is linear because it is linear in the feature $x$\n",
    " - However, what makes a linear model *linear* is that it is linear in terms of the model parameters $w_i$.\n",
    "\n",
    "A quadratic model such as $f(x) = w_0 + w_1x + w_2x^2$ is still linear in terms of the weights. Therefore, this is still a linear model. (It is not linear in the feature $x$ of course).\n",
    "\n",
    "We can extend use of linear models beyond polynomials. We can construct linear models using any nonlinear functions of the features:\n",
    "\n",
    "\\begin{align} f(\\mathbf{x}) & = w_0 + w_1 \\phi_1(\\mathbf{x}) + w_2 \\phi_2(\\mathbf{x}) + \\ldots + w_d \\phi_d (\\mathbf{x}) \\\\ & = \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}) \\end{align}\n",
    "\n",
    "\n",
    "In the labs, you will use another nonlinear function in one of the exercises.\n",
    "\n",
    "We will now use the popular scikit learn library now to construct our linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn import metrics \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w0 =  [11.94393928]\n",
      "coefficients =  [[ 0.00000000e+00 -1.18591601e+00  5.33186772e-01 -1.18779443e-01\n",
      "   1.41384728e-02 -9.54569112e-04  3.66786874e-05 -7.47381273e-07\n",
      "   6.26749407e-09]]\n"
     ]
    }
   ],
   "source": [
    "regressor = LinearRegression()\n",
    "regressor.fit(X,t)\n",
    "\n",
    "# print the intercept (w0)\n",
    "print('w0 = ',regressor.intercept_)\n",
    "  \n",
    "# print the weight vector for the model (w1, w2, ...)\n",
    "print('coefficients = ',regressor.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x_test = np.linspace(0,28,100)[:,None]\n",
    "X_test = np.ones_like(x_test)\n",
    "for i in range(1,maxorder+1):\n",
    "    X_test = np.hstack((X_test,x_test**i))\n",
    "    \n",
    "y_test = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1eefbe55588>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_test,y_test,'b-',linewidth=2)\n",
    "plt.plot(x,t,'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is the same as Figure 1.10 from *A First Course in Machine Learning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04318491950082525\n"
     ]
    }
   ],
   "source": [
    "train_predictions = regressor.predict(X)\n",
    "train_mse = metrics.mean_squared_error(t,train_predictions)\n",
    "print(train_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Compare to simple linear regression model $y = w_0 + w_1x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w0 =  [11.14109659]\n",
      "coefficients =  [[-0.05332354]]\n"
     ]
    }
   ],
   "source": [
    "regressor = LinearRegression()\n",
    "regressor.fit(x,t)\n",
    "\n",
    "# print the intercept (w0)\n",
    "print('w0 = ',regressor.intercept_)\n",
    "  \n",
    "# print the weight vector for the model (w1, w2, ...)\n",
    "print('coefficients = ',regressor.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.050307110475657925\n"
     ]
    }
   ],
   "source": [
    "train_predictions_simple = regressor.predict(x)\n",
    "train_mse_simple = metrics.mean_squared_error(t,train_predictions_simple)\n",
    "print(train_mse_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generalisation and Overfitting\n",
    "\n",
    "- The mean squared error for the degree 8 polynomial model is lower than the mean squared error of the linear model.\n",
    "  - Does this mean it is a better model?\n",
    "\n",
    "- Recall from last week what the aim of learning is: to generalise to *new, unseen* data.\n",
    "- We have only calculated the mean squared error on data we have seen during training.\n",
    "- Looking at the polynomial model of degree order 8, we can see that it makes very spurious predictions for future Olympic games (unseen data, whereas the straight line model seemed more reasonable.\n",
    "- The polynomial model of degree 8 has much more flexibility than the straight line model: \n",
    "  - it is capable of replicating the straight line model by setting coefficients $w_2, w_3, \\ldots $ to zero\n",
    "  - by varying these weights (away from zero) however, it is capable of reducing the training cost further: when this happens too much, we are *over-fitting* to the training data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Validation data\n",
    "\n",
    "- When we have trained a model, we should **validate** it, to ensure it generalizes well to unseen data (and isn't over-fitting to the training data)\n",
    "- For this purpose, we use a second dataset called a **validation set**.\n",
    "- The key property of the validation set is that it is *not seen during training*.\n",
    "- The validation set is usually just a subset of our original dataset, that we set aside and accept won't be used for training.\n",
    "  - The `train_test_split()` method from scikit learn can be used to split the original dataset into training and validation set for us.\n",
    "- To choose between a set of potentials models, we train each one on the training set, and evaluate their loss on the validation set.\n",
    "  - By 'loss', we mean some metric indicating the accuracy of the model. It may well be the same metric that was used fro training, just calculated on the validation set.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![FromSimpleToPoly](Images/FCML_Fig1_12_cite.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cross-validation\n",
    "\n",
    "![FromSimpleToPoly](Images/FCML_Fig1_14_citec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularisation\n",
    "\n",
    "Consider a polynomial model\n",
    "\n",
    "\\begin{equation}\n",
    " f(x) = w_0 + w_1x + w_2x^2 + w_3x^3 + w_4x^4 + w_5x^5\n",
    "\\end{equation}\n",
    "\n",
    "- Generally, the larger the (absolute) values of the weights in the model, the more complex the model is.\n",
    "- Large weights cause more drastic changes in the value of $f(x)$\n",
    "  - For example, consider the derivative of $f$ with respect to $x$: the larger the weights, the larger the derivatives\n",
    "- Therefore, one way to keep models simpler is to keep the absolute values of the parameters low. This is known as **regularisation**. \n",
    "- There are different ways to achieve regularisation, depending on how we capture model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ridge regression\n",
    "\n",
    "- In **ridge regression** we capture model complexity by summing the squares of the model weights: $\\sum_i w_i^2 = \\mathbf{w}^T\\mathbf{w}$\n",
    "- We then adding this to our normal loss function (e.g. least squares) when training, i.e. we minimize:\n",
    "  \\begin{equation} \\frac{1}{N} (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) + \\lambda \\mathbf{w}^T\\mathbf{w}\\end{equation}\n",
    "- The parameter $\\lambda$ (>0) controls the trade-off between model accuracy and model complexity:\n",
    "  - If $\\lambda$ is too small, then complex models are not penalized enough, and training favours high accuracy (potentially leading to over-fitting)\n",
    "  - If $\\lambda$ is too large, then simple models may be favoured too much during training, possibly leading to under-fitting (useful trends are not captured).\n",
    "- A closed form expression exists for ridge regression (like the normal equations), but the problems exists as to how to set the value of $\\lambda$. Usualy we use cross-validation to set its value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hyperparameters\n",
    "\n",
    "The parameter $\\lambda$ in regularisation, and the maximum degree of polynomial in polynomial regression, are both examples of *hyperparameters*.  \n",
    "\n",
    "Hyperparameters are parameters that cannot be optimized directly during training.  Instead another optimization procedure such as cross-validation is needed to determine the best values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature selection\n",
    "\n",
    "- The weights in a linear model give an indication of the features they multiply.\n",
    "- This can be used to simplify models: features with corresponding low weights may be removed from the model with little effect on accuracy.\n",
    "- **Recursive feature elimination (RFE)** is recursively considers smaller and smaller sets of features. \n",
    "  - First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. \n",
    "  - Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
