{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# COM761 Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Teaching staff\n",
    "- Prof Jane Zheng `h.zheng@ulster.ac.uk` (Module Coordinator)\n",
    "- Dr Glenn Hawe `gi.hawe@ulster.ac.uk`\n",
    "\n",
    "### Lab Tutors\n",
    "- Conor Clare `clare-c@ulster.ac.uk` 1-1 hour: Each TUESDAY 3-4 pm\n",
    "- Isaac Ampomah `ampomah-i@ulster.ac.uk` 1-1 hour: Each FRIDAY 2-3 pm\n",
    "- Mohammad Saedi `saedi-m@ulster.ac.uk` 1-1 hour: Each TUESDAY 3-4 pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Weekly Schedule\n",
    "\n",
    "**Each Wednesday**\n",
    "\n",
    "- **2:15 - 3:15** LECTURE (RECORDED)\n",
    "- **3:15 - 3:30** *BREAK* (NOT RECORDED)\n",
    "- **3:30 - 4:30** LECTURE (RECORDED)\n",
    "- **4:30 - 4:40** LAB INTRO (RECORDED)\n",
    "- **4:40 - 5:00** *BREAK*\n",
    "- **5:00 - 8:00** LABS (NOT RECORDED)\n",
    "\n",
    "Feel free to take some time for your dinner, but please try to make the most of the lab sessions, i.e. aim to attend at least 2 hours.\n",
    "\n",
    "The lectures will be recorded and made available on Blackboard shortly afterwards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Teaching Plan (Weeks 1-5)  \n",
    "\n",
    "**Lecturer: Dr Glenn Hawe**\n",
    "\n",
    "**1. Introduction to machine learning** \n",
    "\n",
    "- 1.1: Introduction to Machine Learning\n",
    "- 1.2: Matrices and NumPy\n",
    "- Labs: Exercises involving NumPy and pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**2. Linear regression** \n",
    "\n",
    "- 2.1: Multiple linear regression; non-linear responses; cross-validation; maximum likelihood\n",
    "- 2.2: Alternative cost functions; regularisation; diagnostics\n",
    "- Labs: Exercises in linear regression\n",
    "\n",
    "\n",
    "**3. Optimization for machine learning**\n",
    "\n",
    "- 3.1: Zero-order optimization: Grid search; Random Search; Coordinate Search / Descent\n",
    "- 3.2: First-order optimization: Derivatives; Gradient Descent\n",
    "- Labs: Optimization exercises; evolutionary algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**4. The Bayesian approach to machine learning** \n",
    "\n",
    "- 4.1: Bayesian thinking (priors, likelihood, posterior)\n",
    "- 4.2: Bayesian linear regression\n",
    "- Labs: Probabilistic Programming and Pymc3\n",
    "\n",
    "**5. Advanced Bayesian machine learning** \n",
    "\n",
    "- 5.1: Markov Chain Monte Carlo (MCMC)\n",
    "- 5.2: Gaussian Process regression and Bayesian Optimization \n",
    "- Labs: MCMC diagnostics; GP regression models and Bayesian optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Teaching Plan (Weeks 6-10)  \n",
    "\n",
    "**Lecturer: Prof Jane Zheng**\n",
    "\n",
    "**6. Classification I**: process and algorithms\n",
    "\n",
    "**7. Classification II**: algorithms and assessment\n",
    "\n",
    "**8. Unsupervised Learning I**: concept and algorithms\n",
    "\n",
    "**9. Unsupervised Learning II**: algorithms and assessment\n",
    "\n",
    "**10. Ethical AI, Explainable AI (XAI) & ML**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Assessment\n",
    "\n",
    "This module is assessed entirely by Coursework. There are two pieces of coursework:\n",
    "\n",
    "#### Coursework 1 [40%]\n",
    "\n",
    "- A Jupyter notebook containing four short questions\n",
    "- Covers core material from Weeks 1-3\n",
    "- Will be released in Week 3\n",
    "- Deadline: end of Week 5\n",
    "- Submission will be via Blackboard\n",
    "- An individual assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Coursework 2 [60%]\n",
    "\n",
    "- Three longer questions covering material from Weeks 4-9\n",
    "- An individual assignment\n",
    "- Submission will be via Blackboard.\n",
    "- Deadline: end of Week 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Late submissions\n",
    "\n",
    "- If you are unable to submit due to extenuating circumstances, please submit an EC1 form to the school office.\n",
    "- Otherwise, an assessment submitted late will score a mark of zero.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Plagiarism\n",
    "- Both pieces of coursework are *individual* assignments.\n",
    "- You should not share your solutions with other students for any reason.\n",
    "- If you rely heavily on some source to answer a question, then it should be cited in your code as a comment. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recommended Texts\n",
    "\n",
    "![Books](Images/books3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Useful blogs / websites\n",
    "\n",
    "- https://machinelearningmastery.com/\n",
    "- https://www.kdnuggets.com/\n",
    "- https://distill.pub/\n",
    "- https://towardsdatascience.com/ (paid subscription via medium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Software\n",
    "\n",
    "- We will be using Python\n",
    "- The majority of teaching material will be delivered via Jupyter notebooks\n",
    "- To open and use these Jupyter notebooks you should use either:\n",
    "    - JupyterLab (Installed locally on your machine): \n",
    "       - https://www.anaconda.com/products/individual\n",
    "    - Google colab (Cloud-based approach; no need to install anything):\n",
    "       - https://colab.research.google.com/\n",
    "- Ask in Labs today if you need help getting set-up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is machine learning and how does it fit into AI (and what is deep learning?)\n",
    "\n",
    "![Venn](Images/AI_Venn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Definitions of machine learning\n",
    "\n",
    "**Arthur Samuel** defined machine learning in 1959 as:\n",
    "\n",
    "Machine Learning is the subfield of computer science that gives computers the ability to learn without being explicitly programmed.\n",
    "\n",
    "**Tom Mitchell** coined the following popular definition in 1998:\n",
    "\n",
    "A computer program is said to learn from experience $E$ with respect to some class of tasks $T$ and performance measure $P$, if its performance at tasks in $T$, as measured by $P$, improves with experience $E$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### An Illustrative Example\n",
    "\n",
    "- This example is taken from Chapter 1 of *Machine Learning Refined*.\n",
    "- It will allow us to informally introduce the terminology of machine learning.\n",
    "\n",
    "Suppose we need to teach a computer how to distinguish between pictures of *cats* and *dogs*. \n",
    "\n",
    "- How does a child learn the difference between a cat and a dog? They learn by *example*.\n",
    "- After seeing many cats and dogs, and being told which is which by e.g. their parents (a *supervisor*), the child *learns* how to distinguish between cats and dogs.\n",
    "- How do we know when a child can successfully distinguish between cats and dogs?  When they encounter new cats and dogs and can correctly identify each new example.\n",
    "  - i.e. when they can *generalize* what they have learned to new, previously unseen, examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Computers can be taught how to perform this task in a similar manner.\n",
    "\n",
    "Relating this back to Tom Mitchell's definition of machine learning:\n",
    "- The task $T$ is *classification*: distinguishing between different *classes* of object (in this case, cats and dogs).\n",
    "- The experience $E$ is a set of images of cats and dogs that a supervisor has labelled and the learner has seen.\n",
    "- The performance $P$ is the accuracy with which new, previously unseen, examples, are labelled.\n",
    "\n",
    "We now summarize the main steps involved in this classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### 1. Data Collection\n",
    "\n",
    "Collect a set of labelled images of cats and dogs.\n",
    "\n",
    "![MLR1_01](Images/MLR_Fig1_01.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### 2. Feature design\n",
    "\n",
    "- How do we (humans) tell the difference between cats and dogs?\n",
    "- We use color, size, shape of ears or nose, $\\ldots$ in order to distinguish between the two.\n",
    "    - In machine learning, these are called *features*.\n",
    "- In order to train a computer to perform this task (or more generally, any machine learning task), we need to provide it with properly designed *features*.\n",
    "  - *Representation learning* is a form of machine learning concerned with learning features. \n",
    "  - *Deep learning* is a form of representational learning concerned with learning a hierarchy of features (of varying degrees of abstractness).\n",
    "- For our toy problem of distinguishing cats from dogs, suppose we use two features: \n",
    "  - *size of nose* (relative to the size of the head), ranging from small to large.\n",
    "  - *shape of ears* ranging from round to pointy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![MLR1_02](Images/MLR_Fig1_02b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Representation of the experience (dataset)\n",
    "\n",
    "One common way of describing a dataset is with a **design matrix**. This is a matrix where:\n",
    "- each row contains a different example\n",
    "- each column represents a different feature\n",
    "\n",
    "Most learning algorithms operate on a design matrix representing the dataset. \n",
    "\n",
    "- Therefore to understand machine learning, you should be comfortable using matrices.\n",
    "- Today we will introduce the Python library NumPy for working with matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### 3. Model Training\n",
    "\n",
    "With our feature representation of the training data, the machine learning problem of distinguishing cats and dogs is a geometric one: have the computer find a line or a curve that separates the cats from the dogs in our carefully designed feature space.\n",
    "\n",
    "- If we are to fit a straight line $y=w_0 + w_1x$, then we must find the *best* values for its two parameters:\n",
    "  - $w_1$ the gradient\n",
    "  - $w_0$ the intercept on the vertical axis\n",
    "- This is an **optimization** problem.  **Optimization is key to machine learning, and we will spend Week 3 on this topic.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![MLR1_03](Images/MLR_Fig1_03b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### 4. Model Validation\n",
    "\n",
    "How can we test that we have successfully learnt how to distinguish between cats and dogs?\n",
    "\n",
    "We cannot rely on just classifying the examples in the training set: in this case we would get them all correct! And our model is surely not perfect.\n",
    "\n",
    "Instead we need to test our model using some *unseen* images of cats and dogs. i.e. images that were not used in training the model. Such a set is called a **validation** set.\n",
    "\n",
    "![MLR1_04](Images/MLR_Fig1_04b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![MLR1_05](Images/MLR_Fig1_05b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![MLR1_06](Images/MLR_Fig1_06.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Summary\n",
    "\n",
    "- Machine learning involves learning how to perform a **task** from **experience**.\n",
    "- Experience comes in the form of a collection of **examples**, known as a **dataset**.\n",
    "- Each example in the dataset is defined by its **features**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Supervised vs unsupervised learning\n",
    "\n",
    "- In our toy problem, each example that we learnt from had, in addition to its features, a label (in this case cat/dog).\n",
    "- If every example we learn from has a label, and the task is to learn how the label depends on the values of the features (so that we can then predict the label for new, unseen, unlabelled examples), then the learning is said to be **supervised**. \n",
    "  - When the label is a discrete class, then the task is called **classification**; classification will be covered in Weeks 6-7. \n",
    "  - When the label is continuous (a real number), then the task is called **regression**; we will focus on it until Week 5. \n",
    "- If no example has a label (i.e. we only have feature values), then the task is usually to learn useful properties of the structure of this dataset. This is called **unsupervised learning**. Unsupervised learning will be covered in Weeks 8-9. The most common tasks in unsupervised learning are *dimensionality reduction* and *clustering*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression\n",
    "\n",
    "**Regression** is the task of predicting how a continuous valued target depends on the features.\n",
    "\n",
    "![reg_task](Images/reg_task.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple linear regression\n",
    "\n",
    "We will cover linear regression in more depth next week, after we have covered matrices. \n",
    "\n",
    "Today, we will restrict ourselves to *simple* linear regression, which is fitting a straight line through the data, when we have one single feature. \n",
    "\n",
    "A straight line is defined by the equation $y = w_0 + w_1x$.\n",
    "- Here, $y$ is the label (or target)\n",
    "- $x$ is our single feature\n",
    "- $w_0$ is the intercept on the $y$-axis\n",
    "- $w_1$ is the gradient of the line\n",
    "\n",
    "\n",
    "The parameters of our model are $w_0$ and $w_1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Defining a good model\n",
    "\n",
    "Given a set of data $\\{(x_1,y_1),(x_2,y_2),\\ldots,(x_n,y_n)\\}$, training a simple linear regression model $y = w_0 + w_1x_i$ involves finding the *best* values of $w_0$ and $w_1$.\n",
    "\n",
    "Intuitively, we would expect the *best* line to pass as closely as possible to *all* of the data points.\n",
    "\n",
    "A common way of measuring how close a line is through a set of points is:\n",
    "1. Measure each of the residuals (residual = true value - predicted value)\n",
    " - At $x_i$ the predicted value is $w_0 + w_1x_i$, so the residual is $y_i - (w_0 + w_1x_i)$\n",
    "2. Square each residual (to make them all positive)\n",
    "3. Find the mean (i.e. average) of the squares of all of the residuals\n",
    "\n",
    "This gives us the following **loss function** \n",
    "\n",
    "$g(w_0,w_1) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - (w_0 + w_1x_i))^2$\n",
    "\n",
    "The values of $w_0$ and $w_1$ that minimize $g(w_0,w_1)$ give rise to the *least squares* model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![MLR5_02](Images/MLR_Fig5_02.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Finding the optimal parameter values\n",
    "\n",
    "- The usual approach to finding the optimal parameters of a model is using an *optimization* algorithm to search efficiently through parameter space to minimize the loss function.\n",
    "\n",
    "- We will look at algorithms for optimizing loss functions in Week 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"1000\" height=\"400\" controls loop>\n",
       "  <source src=\"Videos/animation_1.mp4\" type=\"video/mp4\">\n",
       "  </video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Video below from https://github.com/jermwatt/machine_learning_refined\n",
    "# Shared under the Creative Commons Attribution 4.0 International License (CC BY-NC-SA 4.0)\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"1000\" height=\"400\" controls loop>\n",
    "  <source src=\"Videos/animation_1.mp4\" type=\"video/mp4\">\n",
    "  </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Normal equations\n",
    "\n",
    "- In the case of linear regression, the optimal values can be determined analytically, by differentiating the loss function and solving for when the gradient is equal to zero.\n",
    "\n",
    "- We refer to p. 8-14 *A First Course in Machine Learning*  by Simon Rogers and Mark Girolami for details of the derivation, but the optimal values $\\hat{w_0}$ and $\\hat{w_1}$ of $w_0$ and $w_1$ are:\n",
    "\n",
    "![normeq](Images/simplelinearnormeq2.png)\n",
    "\n",
    "where a bar over a term means the average value for that term (calculated from the training data)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
