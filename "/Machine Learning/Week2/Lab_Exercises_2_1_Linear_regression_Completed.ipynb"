{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Lab Exercises 2.1 Linear regression.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ofithcheallaigh/uu_pgc_ai/blob/main/%22/Machine%20Learning/Week2/Lab_Exercises_2_1_Linear_regression_Completed.ipynb%22\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-y-ROH4XdP5"
      },
      "source": [
        "# Lab Exercises Week 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2Lm2YOUXdP7"
      },
      "source": [
        "# Exercise 1: Overfitting and Cross Validation\n",
        "\n",
        "Use `LinearRegression` from scikit learn to produce Fig 1.12 (a) below from *A First Course in Machine Learning*, using the Olympic data. \n",
        "\n",
        "Hints:\n",
        "- you will need to create a validation set, using the data from 1980 onwards (the last 8 rows in the dataset)\n",
        "- all the rest of the data should be used for training.\n",
        "- the *Training Loss* on the y-axis is **not** the mean squared loss, but the *total* squared loss (I don't think this is said anywhere in the book though!)\n",
        "- scale the data as was done in the lecture, to avoid problems with high-order polynomials\n",
        "\n",
        "![FromSimpleToPoly](https://drive.google.com/uc?id=1fv99Hf5E9CuaxLT-2ZjSvYw-m_7w-bQv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejofSUBoXdP7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtnl8FzOXdQB"
      },
      "source": [
        "## Exercise 1b\n",
        "\n",
        "Repeat the above making use of:\n",
        "\n",
        "`from sklearn.preprocessing import PolynomialFeatures`\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\n",
        "\n",
        "See also here for a worked example:\n",
        "https://machinelearningmastery.com/polynomial-features-transforms-for-machine-learning/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE1Wqqo5XdQB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGYjjqT3XdQF"
      },
      "source": [
        "# Exercise 2: Nonlinear response from linear model\n",
        "\n",
        "Use `LinearRegression` from scikit learn to produce Fig 1.11 below from *A First Course in Machine Learning*, using the Olympic data.\n",
        "\n",
        "Hints: \n",
        "- You will need to create a design matrix $\\mathbf{X}$ containing 3 columns: a column of 1s; a column containing $x$ (the year); and a column containing $\\sin \\left(\\frac{x-a}{b}\\right)$ (where $a = 2660$ and $b = 4.3$).\n",
        "- All data is used for training in this plot (i.e. none is withheld in a validation set).\n",
        "\n",
        "![FromSimpleToPoly](Images/FCML_Fig1_11.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5L6NFVeXdQF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0u-8lAzXdQJ"
      },
      "source": [
        "# Exercise 3: Regularisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeeBtShuXdQJ"
      },
      "source": [
        "Replicate Figure 1.17 from *A First Course in Machine Learning*.  It is fitting a fifth order polynomial to six points, with regularisation.\n",
        "\n",
        "Use the points:\n",
        "\n",
        "$(0,-3.2),(0.2,-4.8),(0.4,3),(0.6,-0.2),(0.8,1),(1,-3)$\n",
        "\n",
        "Use `Ridge` from:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge\n",
        "\n",
        "Also you may want to try out:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV\n",
        "\n",
        "Note what we have called $\\lambda$, is referred to as `alpha` in `Ridge` in scikit learn.\n",
        "\n",
        "![FromSimpleToPoly](Images/FCML_Fig1_17_cite.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LEXOrL2XdQJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4el23e50XdQL"
      },
      "source": [
        "# Exercise 4: Multiple Linear regression and Feature Selection\n",
        "\n",
        "Use `LinearRegression` from scikit learn to predict the quality of wine in the wine data set from last week.\n",
        "\n",
        "Start by building a model that includes all the features.\n",
        "\n",
        "Then use Recursive Feature Selection in scikit learn to build a model with the best *five* features.\n",
        "\n",
        "Suggested reading:\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE\n",
        "\n",
        "https://machinelearningmastery.com/rfe-feature-selection-in-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnVsjwNrXdQM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzFJ8_seXdQN"
      },
      "source": [
        "# Exercise 5: Some reading involving linear regression and optimization\n",
        "\n",
        "Read: https://machinelearningmastery.com/implement-linear-regression-stochastic-gradient-descent-scratch-python/\n",
        "\n",
        "This describes an implementation of multiple linear regression, as well as providing a brief intro to a gradient descent optimization algorithm."
      ]
    }
  ]
}